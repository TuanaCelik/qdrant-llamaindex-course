{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Event-Driven Agentic RAG with Qdrant and LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to use Qdrant as your vector database for agentic AI workflows built with LlamaIndex. In this mini course, we will be walking through building a simple agent workflow that can make decisions about:\n",
    "- querying a Qdrant collection, \n",
    "- or writing new statements to it.\n",
    "For this first step, we will be using a local Qdrant collection that we run as a Docker container on our machine. \n",
    "\n",
    "Finally, we will see how you can also make use of a managed Qdrant collection as a sink in LlamaCloud\n",
    "\n",
    "<img src=\"assets/workflow.png\" alt=\"drawing\" style=\"width:500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install qdrant-client fastembed llama-index-vector-stores-qdrant llama-index-embeddings-openai llama-index-llms-openai python-dotenv llama-index llama-index-utils-workflow llama-index-readers-web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, you should set up a `.env` file containing environment variables needed for the pipeline to run properly, such as the `OPENAI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to a Local Qdrant Collection\n",
    "\n",
    "To get started:\n",
    "```bash\n",
    "docker pull qdrant/qdrant\n",
    "docker run -d --name qdrant -p 6333:6333 -p 6334:6334 qdrant/qdrant:latest\n",
    "```\n",
    "\n",
    "We can make use of hybrid dense & sparse retrieval when using Qdrant. Here, while setting up the `QdrantVectorStore`, we set our sparse embedding model too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "\n",
    "if client.collection_exists(\"my_collection\"):\n",
    "    client.delete_collection(\"my_collection\")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    collection_name=\"my_collection\",\n",
    "    client=client,\n",
    "    fastembed_sparse_model=\"Qdrant/minicoil-v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.core import Settings\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=32)\n",
    "Settings.node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"LlamaIndex is a simple, flexible data framework for connecting custom data sources to large language models.\",\n",
    "        metadata={\n",
    "            \"library\": \"llama-index\",\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"Tuana is DevRel at LlamaIndex.\",\n",
    "        metadata={\n",
    "            \"library\": \"llama-index\",\n",
    "        },\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"Qdrant is a vector database & vector similarity search engine.\",\n",
    "        metadata={\n",
    "            \"library\": \"qdrant\",\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    vector_store=vector_store,\n",
    "    embed_model=OpenAIEmbedding(),\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Over Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qdrant is a vector database and vector similarity search engine.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"What is Qdrant?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an Agent Workflow \n",
    "\n",
    "In this example, we will be building an agent workflow that acts as both an agentic RAG application, as well as a database management tool. Our aim is to make use of [LlamaIndex Workflows](https://docs.llamaindex.ai/en/stable/module_guides/workflow/) to create a workflow that has a decision step, which based on the users query, makes the agent either query the database, or write to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Function Calling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_statement(statement: str) -> str:\n",
    "    \"\"\"Useful for writing statements to a collection\"\"\"\n",
    "    document = Document(text=statement)\n",
    "    await index.ainsert_nodes([document])\n",
    "    return f\"Wrote the statement: {statement} to the collection\"\n",
    "\n",
    "def query_collection(query: str) -> str:\n",
    "    \"\"\"Useful for querying the collection\"\"\"\n",
    "    response = await query_engine.aquery(query)\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "llm = OpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=[write_statement, query_collection],\n",
    "    llm=llm,\n",
    "    system_prompt=\"\"\"You are a helpful assistant that can write statements to\n",
    "    a collectoin or forward queries to it.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "async def view_agent_steps(agent, input:str):\n",
    "    handler = agent.run(input)\n",
    "    current_agent = None\n",
    "    current_tool_calls = \"\"\n",
    "    async for event in handler.stream_events():\n",
    "        if (\n",
    "            hasattr(event, \"current_agent_name\")\n",
    "            and event.current_agent_name != current_agent\n",
    "        ):\n",
    "            current_agent = event.current_agent_name\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ðŸ¤– Agent: {current_agent}\")\n",
    "            print(f\"{'='*50}\\n\")\n",
    "        elif isinstance(event, AgentOutput):\n",
    "            if event.response.content:\n",
    "                print(\"ðŸ“¤ Output:\", event.response.content)\n",
    "            if event.tool_calls:\n",
    "                print(\n",
    "                    \"ðŸ› ï¸  Planning to use tools:\",\n",
    "                    [call.tool_name for call in event.tool_calls],\n",
    "                )\n",
    "        elif isinstance(event, ToolCallResult):\n",
    "            print(f\"ðŸ”§ Tool Result ({event.tool_name}):\")\n",
    "            print(f\"  Arguments: {event.tool_kwargs}\")\n",
    "            print(f\"  Output: {event.tool_output}\")\n",
    "        elif isinstance(event, ToolCall):\n",
    "            print(f\"ðŸ”¨ Calling Tool: {event.tool_name}\")\n",
    "            print(f\"  With arguments: {event.tool_kwargs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ðŸ¤– Agent: Agent\n",
      "==================================================\n",
      "\n",
      "ðŸ› ï¸  Planning to use tools: ['query_collection']\n",
      "ðŸ”¨ Calling Tool: query_collection\n",
      "  With arguments: {'query': 'Who is Tuana?'}\n",
      "ðŸ”§ Tool Result (query_collection):\n",
      "  Arguments: {'query': 'Who is Tuana?'}\n",
      "  Output: Tuana is DevRel at LlamaIndex.\n",
      "ðŸ“¤ Output: Tuana is a Developer Relations (DevRel) professional at LlamaIndex. If you would like to know more about Tuana or their work, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "await view_agent_steps(agent, \"Who is Tuana?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Custom Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Union\n",
    "\n",
    "from llama_index.core.workflow import (Workflow, Event, step, StartEvent, StopEvent)\n",
    "from llama_index.llms.openai import OpenAIResponses\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "class SaveToDocs(BaseModel):\n",
    "\t\"\"\"The statement to save into your collection.\"\"\"\n",
    "\tstatement: str = Field(default_factory=str)\n",
    "\n",
    "\n",
    "class Ask(BaseModel):\n",
    "\t\"\"\"The natural language questions that can be asked to a Q&A agent.\"\"\"\n",
    "\tqueries: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class Actions(BaseModel):\n",
    "\t\"\"\"Actions to take based on the latest user message.\"\"\"\n",
    "\tactions: List[Union[SaveToDocs, Ask]] = Field(default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriteStatement(Event):\n",
    "\tstatement: str\n",
    "\n",
    "class QueryIndex(Event):\n",
    "\tqueries: List[str]\n",
    "\n",
    "class QdrantDocumentAgent(Workflow):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tself.sllm = OpenAIResponses(model=\"gpt-4.1-mini\").as_structured_llm(Actions)\n",
    "\t\tself.system_prompt = \"\"\"You are a docs assistant. You evaluate incoming queries and break them down to subqueries when needed.\n",
    "\t\t\t\t\t\t\t\tYou decide on the next best course of action. Overall, here are the options:\n",
    "\t\t\t\t\t\t\t\t- You can write documents to your collection.\n",
    "\t\t\t\t\t\t\t\t- You can answer a questions based on the contents of your collection.\"\"\"\n",
    "\t\tsuper().__init__(*args, **kwargs)\n",
    "\n",
    "\t@step\n",
    "\tasync def start(self, ev: StartEvent) -> WriteStatement | QueryIndex:\n",
    "\n",
    "\t\tresponse = await self.sllm.achat(\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t\tChatMessage(role=\"system\", content=self.system_prompt),\n",
    "\t\t\t\t\t\tChatMessage(role=\"user\", content=ev.query),\n",
    "\t\t\t\t]\n",
    "\t\t)\n",
    "\t\tactions = response.raw.actions\n",
    "\t\tprint(actions)\n",
    "\t\tfor action in actions:\n",
    "\t\t\tif isinstance(action, SaveToDocs):\n",
    "\t\t\t\tprint(\"Got Save event\")\n",
    "\t\t\t\treturn WriteStatement(statement=action.statement)\n",
    "\t\t\telif isinstance(action, Ask):\n",
    "\t\t\t\tprint(\"Got Ask event\")\n",
    "\t\t\t\treturn QueryIndex(queries=action.queries)\n",
    "\t@step\n",
    "\tasync def query_index(self, ev: QueryIndex) -> StopEvent:\n",
    "\t\tprint(f\"Request to query index with queries: {ev.queries}\")\n",
    "\t\treturn StopEvent()\n",
    "\n",
    "\t@step\n",
    "\tasync def save_to_index(self, ev: WriteStatement) -> StopEvent:\n",
    "\t\tprint(f\"Request to write to index: {ev.statement}\")\n",
    "\t\treturn StopEvent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = QdrantDocumentAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ask(queries=['Who is Kacper?'])]\n",
      "Got Ask event\n",
      "Request to query index with queries: ['Who is Kacper?']\n"
     ]
    }
   ],
   "source": [
    "respone = await workflow.run(start_event=StartEvent(query=\"Who is Kacper?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantDocumentAgent(Workflow):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "        self.sllm = OpenAIResponses(model=\"gpt-4.1-mini\").as_structured_llm(Actions)\n",
    "        self.system_prompt = \"\"\"You are a docs assistant. You evaluate incoming queries and break them down to subqueries when needed.\n",
    "                          You decide on the next best course of action. Overall, here are the options:\n",
    "                          - You can write documents to your collection.\n",
    "                          - You can answer a questions based on the contents of your collection.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "  @step\n",
    "  async def start(self, ev: StartEvent) -> WriteStatement | QueryIndex:\n",
    "    response = await self.sllm.achat(\n",
    "        [\n",
    "            ChatMessage(role=\"system\", content=self.system_prompt),\n",
    "            ChatMessage(role=\"user\", content=ev.query),\n",
    "        ]\n",
    "    )\n",
    "    actions = response.raw.actions\n",
    "    print(actions)\n",
    "    for action in actions:\n",
    "      if isinstance(action, SaveToDocs):\n",
    "          return WriteStatement(statement=action.statement)\n",
    "      elif isinstance(action, Ask):\n",
    "          return QueryIndex(queries=action.queries)\n",
    "\n",
    "  @step\n",
    "  async def query_index(self, ev: QueryIndex) -> StopEvent:\n",
    "    for query in ev.queries:\n",
    "      response = query_engine.query(query)\n",
    "      print(response)\n",
    "    return StopEvent()\n",
    "\n",
    "  @step\n",
    "  async def save_to_index(self, ev: WriteStatement) -> StopEvent:\n",
    "    document = Document(text=ev.statement)\n",
    "    await index.ainsert_nodes([document])\n",
    "    print(f\"Wrote {document} to the index\")\n",
    "    return StopEvent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SaveToDocs(statement='Kacper is DevRel at Qdrant.')]\n",
      "Wrote Doc ID: 4c009c95-eeb2-46ee-87a5-c2384ae51f1c\n",
      "Text: Kacper is DevRel at Qdrant. to the index\n"
     ]
    }
   ],
   "source": [
    "workflow = QdrantDocumentAgent()\n",
    "response = await workflow.run(start_event=StartEvent(query=\"Write this statement: Kacper is DevRel at Qdrant.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring Real-World Data with Readers and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "async def write_webpages_to_qdrant(index, urls: List[str]):\n",
    "    documents = SimpleWebPageReader(html_to_text=True).load_data(urls)\n",
    "    await index.ainsert_nodes([documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await write_webpages_to_qdrant(index, urls=[\"https://docs.llamaindex.ai/en/stable/module_guides/workflow/\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LlamaCloud and Managed Qdrant Collections\n",
    "\n",
    "So far, we saw how we can build agentic workflows over locally hosted Qdrant collections. But, you can also use managed Qdrant collections that you may have in Qdrant Cloud.\n",
    "\n",
    "By combining LlamaCloud and Qdrant as the sink, you also make use of LlamaCloud's advanced parsing capabilities. You can see the documentation on how to do it programmatically and throught the [UI](https://cloud.llamaindex.ai?utm_source=demo&utm_medium=li_social&utm_campaign=cloud) at on [LlamaCloud Documentation](https://docs.cloud.llamaindex.ai/llamacloud/integrations/data_sinks/qdrant?utm_source=demo&utm_medium=li_social&utm_campaign=cloud).\n",
    "\n",
    "Below, let's see an example of an index, hooked up to Qdrant as the vector database which we manage separately, and a Google Drive folder as the data source ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.indices.managed.llama_cloud import LlamaCloudIndex\n",
    "\n",
    "\n",
    "index = LlamaCloudIndex(\"coming-sole-2025-07-09\", project_name=\"Default\")\n",
    "query_engine = index.as_query_engine()\n",
    "answer = await query_engine.aquery(\"What are LlamaIndex Workflows?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex Workflows are event-driven abstractions used to chain together several events. They are composed of steps, where each step is responsible for handling specific event types and emitting new events. Workflows in LlamaIndex are created by decorating functions with a @step decorator, which helps infer the input and output types of each workflow for validation. These workflows ensure that each step only runs when an accepted event is ready. Additionally, Workflows in LlamaIndex are versatile and can be used to build agents, RAG flows, extraction flows, or any other desired functionality. They are also automatically instrumented for observability, allowing users to gain insights into each step using tools like Arize Phoenix.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "qdrant-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
